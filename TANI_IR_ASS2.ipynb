{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "acf744cd-053d-475e-b1e3-4870f3ee82ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your search query:  Developing your Zomato business account and profile is a great way to boost your  restaurantâ€™s online reputation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top relevant documents:\n",
      "1. (zomato.txt, 0.20259462898399572)\n",
      "2. (swiggy.txt, 0.1137687853973905)\n",
      "3. (instagram.txt, 0.0464958687255616)\n",
      "4. (messenger.txt, 0.04264100643310696)\n",
      "5. (HP.txt, 0.03619127222176074)\n",
      "6. (bing.txt, 0.0323311931258012)\n",
      "7. (youtube.txt, 0.030700663297177078)\n",
      "8. (flipkart.txt, 0.026232317571210088)\n",
      "9. (reddit.txt, 0.024544366934365465)\n",
      "10. (Uber.txt, 0.021324856341440546)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Preprocessing function\n",
    "def preprocess_document(doc):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    # Case folding\n",
    "    doc = doc.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    \n",
    "    # Remove stop words and non-alphanumeric characters\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and token.isalnum()]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Read Documents from Directory\n",
    "def read_documents_from_directory(directory):\n",
    "    documents = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        if os.path.isfile(filepath):\n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                documents[filename] = file.read()\n",
    "    return documents\n",
    "\n",
    "# Build Term-Document Matrix (TF-IDF)\n",
    "def build_term_document_matrix(documents):\n",
    "    N = len(documents)  # Total number of documents\n",
    "    term_document_matrix = defaultdict(lambda: defaultdict(int))\n",
    "    document_frequencies = defaultdict(int)\n",
    "    document_lengths = defaultdict(float)\n",
    "    \n",
    "    # Calculate term frequencies and document frequencies\n",
    "    for doc_id, doc_content in documents.items():\n",
    "        tokens = preprocess_document(doc_content)\n",
    "        term_counts = defaultdict(int)\n",
    "        \n",
    "        for token in tokens:\n",
    "            term_counts[token] += 1\n",
    "        \n",
    "        for term, count in term_counts.items():\n",
    "            term_document_matrix[term][doc_id] = 1 + math.log10(count)  # log(tf)\n",
    "            document_frequencies[term] += 1\n",
    "    \n",
    "    # Calculate document lengths (for cosine normalization)\n",
    "    for term, doc_list in term_document_matrix.items():\n",
    "        idf = math.log10(N / document_frequencies[term])  # log(N/df)\n",
    "        for doc_id, tf in doc_list.items():\n",
    "            term_document_matrix[term][doc_id] = tf * idf  # tf * idf\n",
    "            document_lengths[doc_id] += (term_document_matrix[term][doc_id]) ** 2\n",
    "    \n",
    "    # Normalize document lengths\n",
    "    for doc_id in document_lengths:\n",
    "        document_lengths[doc_id] = math.sqrt(document_lengths[doc_id])\n",
    "    \n",
    "    return term_document_matrix, document_lengths, document_frequencies, N\n",
    "\n",
    "# Normalize query vector\n",
    "def normalize_query(query_terms, term_document_matrix, document_frequencies, N):\n",
    "    query_term_counts = defaultdict(int)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        query_term_counts[term] += 1\n",
    "    \n",
    "    query_vector = {}\n",
    "    for term, count in query_term_counts.items():\n",
    "        tf = 1 + math.log10(count)\n",
    "        idf = math.log10(N / document_frequencies[term]) if term in document_frequencies else 0\n",
    "        query_vector[term] = tf * idf\n",
    "    \n",
    "    query_length = math.sqrt(sum([v ** 2 for v in query_vector.values()]))\n",
    "    \n",
    "    # Normalize the query vector\n",
    "    if query_length > 0:\n",
    "        for term in query_vector:\n",
    "            query_vector[term] /= query_length\n",
    "    \n",
    "    return query_vector\n",
    "\n",
    "# Cosine Similarity Calculation\n",
    "def cosine_similarity(query_vector, term_document_matrix, document_lengths):\n",
    "    scores = defaultdict(float)\n",
    "    \n",
    "    for term, query_weight in query_vector.items():\n",
    "        if term in term_document_matrix:\n",
    "            for doc_id, doc_weight in term_document_matrix[term].items():\n",
    "                scores[doc_id] += query_weight * doc_weight\n",
    "    \n",
    "    for doc_id in scores:\n",
    "        scores[doc_id] /= document_lengths[doc_id]  # Cosine normalization\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Search Function to retrieve top 10 documents\n",
    "def search(query, term_document_matrix, document_lengths, document_frequencies, N):\n",
    "    query_terms = preprocess_document(query)\n",
    "    query_vector = normalize_query(query_terms, term_document_matrix, document_frequencies, N)\n",
    "    \n",
    "    scores = cosine_similarity(query_vector, term_document_matrix, document_lengths)\n",
    "    \n",
    "    # Sort by score and return top 10 results\n",
    "    ranked_docs = sorted(scores.items(), key=lambda item: (-item[1], item[0]))[:10]\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    directory = \"C:\\\\Users\\\\ASUS\\\\Desktop\\\\Corpus\"\n",
    "    \n",
    "    # Read all documents from the directory\n",
    "    documents = read_documents_from_directory(directory)\n",
    "    \n",
    "\n",
    "    term_document_matrix, document_lengths, document_frequencies, N = build_term_document_matrix(documents)\n",
    "    \n",
    "    query = input(\"Enter your search query: \")\n",
    "    \n",
    "    results = search(query, term_document_matrix, document_lengths, document_frequencies, N)\n",
    "    \n",
    "    print(\"Top relevant documents:\")\n",
    "    for rank, (doc_id, score) in enumerate(results, 1):\n",
    "        print(f\"{rank}. ({doc_id}, {score})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f7e0a-5f84-4486-9b2e-4dc7d70f9ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
